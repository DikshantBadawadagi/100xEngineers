{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DikshantBadawadagi/100xEngineers/blob/main/InShorts-WebScrape-News.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6B3yYjVBeh4X",
        "outputId": "725ecc75-318b-4c6e-95fb-9737235bc26e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.10/dist-packages (3.0.3)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.10/dist-packages (3.1.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (5.3.0)\n",
            "Requirement already satisfied: MarkupSafe in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
            "Requirement already satisfied: Werkzeug in /usr/local/lib/python3.10/dist-packages (3.1.3)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Collecting gunicorn\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flask-cors\n",
            "  Downloading Flask_Cors-5.0.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (2024.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from Flask) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from Flask) (1.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gunicorn) (24.2)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Flask_Cors-5.0.0-py2.py3-none-any.whl (14 kB)\n",
            "Installing collected packages: gunicorn, bs4, flask-cors\n",
            "Successfully installed bs4-0.0.2 flask-cors-5.0.0 gunicorn-23.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install beautifulsoup4 Flask Jinja2 lxml MarkupSafe requests urllib3 Werkzeug bs4 gunicorn flask-cors pytz\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import uuid\n",
        "import requests\n",
        "import pytz\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "headers = {\n",
        "    'authority': 'inshorts.com',\n",
        "    'accept': '*/*',\n",
        "    'accept-language': 'en-GB,en;q=0.5',\n",
        "    'content-type': 'application/json',\n",
        "    'referer': 'https://inshorts.com/en/read',\n",
        "    'sec-ch-ua': '\"Not/A)Brand\";v=\"99\", \"Brave\";v=\"115\", \"Chromium\";v=\"115\"',\n",
        "    'sec-ch-ua-mobile': '?0',\n",
        "    'sec-ch-ua-platform': '\"macOS\"',\n",
        "    'sec-fetch-dest': 'empty',\n",
        "    'sec-fetch-mode': 'cors',\n",
        "    'sec-fetch-site': 'same-origin',\n",
        "    'sec-gpc': '1',\n",
        "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',\n",
        "}\n",
        "\n",
        "# Function to fetch news in pages\n",
        "def fetch_news_page(category, page=1, max_limit=100):\n",
        "    params = {\n",
        "        'category': category,\n",
        "        'max_limit': str(max_limit),\n",
        "        'include_card_data': 'true',\n",
        "        'page': str(page)\n",
        "    }\n",
        "    response = requests.get(f'https://inshorts.com/api/en/search/trending_topics/{category}', headers=headers, params=params)\n",
        "    if response.status_code == 200:\n",
        "        try:\n",
        "            news_data = response.json()['data']['news_list']\n",
        "            return news_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing JSON: {e}\")\n",
        "            return []\n",
        "    else:\n",
        "        print(f\"Error fetching page {page}: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "# Function to collect news until the required number is fetched, up to 100 pages\n",
        "def get_large_dataset(category, total_count=100000, max_limit=100, max_pages=100):\n",
        "    news_list = []\n",
        "    page = 1\n",
        "    while len(news_list) < total_count and page <= max_pages:\n",
        "        print(f\"Fetching page {page}... (Collected: {len(news_list)} articles)\")\n",
        "        news_data = fetch_news_page(category, page=page, max_limit=max_limit)\n",
        "        if not news_data:\n",
        "            print(f\"No data on page {page}, skipping...\")\n",
        "            continue  # Skip to the next page if no data is returned\n",
        "        news_list.extend(news_data)\n",
        "        page += 1\n",
        "\n",
        "    return news_list[:total_count]  # Return only the requested number of news items\n",
        "\n",
        "def process_news_data(news_data):\n",
        "    newsDictionary = {\n",
        "        'success': True,\n",
        "        'category': 'business',  # Example category, modify as needed\n",
        "        'data': []\n",
        "    }\n",
        "\n",
        "    for entry in news_data:\n",
        "        try:\n",
        "            news = entry['news_obj']\n",
        "            author = news['author_name']\n",
        "            title = news['title']\n",
        "            imageUrl = news['image_url']\n",
        "            url = news['shortened_url']\n",
        "            content = news['content']\n",
        "            timestamp = news['created_at'] / 1000\n",
        "            dt_utc = datetime.datetime.utcfromtimestamp(timestamp)\n",
        "            tz_utc = pytz.timezone('UTC')\n",
        "            dt_utc = tz_utc.localize(dt_utc)\n",
        "            tz_ist = pytz.timezone('Asia/Kolkata')\n",
        "            dt_ist = dt_utc.astimezone(tz_ist)\n",
        "            date = dt_ist.strftime('%A, %d %B, %Y')\n",
        "            time = dt_ist.strftime('%I:%M %p').lower()\n",
        "            readMoreUrl = news['source_url']\n",
        "\n",
        "            newsObject = {\n",
        "                'id': uuid.uuid4().hex,\n",
        "                'title': title,\n",
        "                'imageUrl': imageUrl,\n",
        "                'url': url,\n",
        "                'content': content,\n",
        "                'author': author,\n",
        "                'date': date,\n",
        "                'time': time,\n",
        "                'readMoreUrl': readMoreUrl\n",
        "            }\n",
        "            newsDictionary['data'].append(newsObject)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing entry: {e}\")\n",
        "    return newsDictionary\n",
        "\n",
        "# Function to filter news based on title query\n",
        "def filter_news_by_title(news_data, query):\n",
        "    filtered_data = [news for news in news_data if query.lower() in news['title'].lower()]\n",
        "    return filtered_data\n",
        "\n",
        "# Function to save data to a CSV file\n",
        "def save_to_csv(data, filename=\"filtered_news_data.csv\"):\n",
        "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=data[0].keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "# Function to convert filtered news to DataFrame\n",
        "def convert_to_dataframe(filtered_data):\n",
        "    df = pd.DataFrame(filtered_data)\n",
        "    return df\n",
        "\n",
        "# Example usage:\n",
        "category = 'business'  # Change to any desired category\n",
        "news_data = get_large_dataset(category, total_count=100000, max_pages=100)  # Fetch news up to 100 pages\n",
        "processed_data = process_news_data(news_data)\n",
        "\n",
        "# Ask the user for a title to search\n",
        "user_query = input(\"Enter the title (e.g., 'Gautam Adani') to search for in the news: \")\n",
        "\n",
        "# Filter the news based on the user query\n",
        "filtered_news = filter_news_by_title(processed_data['data'], user_query)\n",
        "\n",
        "# Convert filtered news to DataFrame\n",
        "df_filtered_news = convert_to_dataframe(filtered_news)\n",
        "\n",
        "# Display the dataframe (optional, for debugging)\n",
        "print(df_filtered_news.head())\n",
        "\n",
        "# Save to CSV (optional)\n",
        "save_to_csv(filtered_news, filename=\"filtered_business_news.csv\")\n",
        "\n",
        "print(\"Filtered news data saved to CSV successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e49k8djzerXl",
        "outputId": "7cd9c821-22f4-41b9-a38b-ea1e67d8330f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching page 1... (Collected: 0 articles)\n",
            "Fetching page 2... (Collected: 10 articles)\n",
            "Fetching page 3... (Collected: 20 articles)\n",
            "Fetching page 4... (Collected: 30 articles)\n",
            "Fetching page 5... (Collected: 40 articles)\n",
            "Fetching page 6... (Collected: 50 articles)\n",
            "Fetching page 7... (Collected: 60 articles)\n",
            "Fetching page 8... (Collected: 70 articles)\n",
            "Fetching page 9... (Collected: 80 articles)\n",
            "Fetching page 10... (Collected: 90 articles)\n",
            "Fetching page 11... (Collected: 100 articles)\n",
            "Fetching page 12... (Collected: 110 articles)\n",
            "Fetching page 13... (Collected: 120 articles)\n",
            "Fetching page 14... (Collected: 130 articles)\n",
            "Fetching page 15... (Collected: 140 articles)\n",
            "Fetching page 16... (Collected: 149 articles)\n",
            "Fetching page 17... (Collected: 159 articles)\n",
            "Fetching page 18... (Collected: 169 articles)\n",
            "Fetching page 19... (Collected: 179 articles)\n",
            "Fetching page 20... (Collected: 189 articles)\n",
            "Fetching page 21... (Collected: 198 articles)\n",
            "Fetching page 22... (Collected: 208 articles)\n",
            "Fetching page 23... (Collected: 218 articles)\n",
            "Fetching page 24... (Collected: 228 articles)\n",
            "Fetching page 25... (Collected: 238 articles)\n",
            "Fetching page 26... (Collected: 248 articles)\n",
            "Fetching page 27... (Collected: 258 articles)\n",
            "Fetching page 28... (Collected: 268 articles)\n",
            "Fetching page 29... (Collected: 277 articles)\n",
            "Fetching page 30... (Collected: 287 articles)\n",
            "Fetching page 31... (Collected: 297 articles)\n",
            "Fetching page 32... (Collected: 307 articles)\n",
            "Fetching page 33... (Collected: 317 articles)\n",
            "Fetching page 34... (Collected: 326 articles)\n",
            "Fetching page 35... (Collected: 336 articles)\n",
            "Fetching page 36... (Collected: 346 articles)\n",
            "Fetching page 37... (Collected: 356 articles)\n",
            "Fetching page 38... (Collected: 366 articles)\n",
            "Fetching page 39... (Collected: 376 articles)\n",
            "Fetching page 40... (Collected: 386 articles)\n",
            "Fetching page 41... (Collected: 396 articles)\n",
            "Fetching page 42... (Collected: 406 articles)\n",
            "Fetching page 43... (Collected: 416 articles)\n",
            "Fetching page 44... (Collected: 426 articles)\n",
            "Fetching page 45... (Collected: 436 articles)\n",
            "Fetching page 46... (Collected: 446 articles)\n",
            "Fetching page 47... (Collected: 456 articles)\n",
            "Fetching page 48... (Collected: 466 articles)\n",
            "Fetching page 49... (Collected: 475 articles)\n",
            "Fetching page 50... (Collected: 485 articles)\n",
            "Fetching page 51... (Collected: 495 articles)\n",
            "Fetching page 52... (Collected: 505 articles)\n",
            "Fetching page 53... (Collected: 514 articles)\n",
            "Fetching page 54... (Collected: 524 articles)\n",
            "Fetching page 55... (Collected: 534 articles)\n",
            "Fetching page 56... (Collected: 544 articles)\n",
            "Fetching page 57... (Collected: 554 articles)\n",
            "Fetching page 58... (Collected: 564 articles)\n",
            "Fetching page 59... (Collected: 574 articles)\n",
            "Fetching page 60... (Collected: 584 articles)\n",
            "Fetching page 61... (Collected: 594 articles)\n",
            "Fetching page 62... (Collected: 604 articles)\n",
            "Fetching page 63... (Collected: 614 articles)\n",
            "Fetching page 64... (Collected: 624 articles)\n",
            "Fetching page 65... (Collected: 634 articles)\n",
            "Fetching page 66... (Collected: 644 articles)\n",
            "Fetching page 67... (Collected: 654 articles)\n",
            "Fetching page 68... (Collected: 663 articles)\n",
            "Fetching page 69... (Collected: 673 articles)\n",
            "Fetching page 70... (Collected: 683 articles)\n",
            "Fetching page 71... (Collected: 693 articles)\n",
            "Fetching page 72... (Collected: 703 articles)\n",
            "Fetching page 73... (Collected: 713 articles)\n",
            "Fetching page 74... (Collected: 722 articles)\n",
            "Fetching page 75... (Collected: 732 articles)\n",
            "Fetching page 76... (Collected: 742 articles)\n",
            "Fetching page 77... (Collected: 752 articles)\n",
            "Fetching page 78... (Collected: 762 articles)\n",
            "Fetching page 79... (Collected: 772 articles)\n",
            "Fetching page 80... (Collected: 782 articles)\n",
            "Fetching page 81... (Collected: 792 articles)\n",
            "Fetching page 82... (Collected: 802 articles)\n",
            "Fetching page 83... (Collected: 812 articles)\n",
            "Fetching page 84... (Collected: 822 articles)\n",
            "Fetching page 85... (Collected: 832 articles)\n",
            "Fetching page 86... (Collected: 842 articles)\n",
            "Fetching page 87... (Collected: 852 articles)\n",
            "Fetching page 88... (Collected: 861 articles)\n",
            "Fetching page 89... (Collected: 871 articles)\n",
            "Fetching page 90... (Collected: 881 articles)\n",
            "Fetching page 91... (Collected: 891 articles)\n",
            "Fetching page 92... (Collected: 900 articles)\n",
            "Fetching page 93... (Collected: 910 articles)\n",
            "Fetching page 94... (Collected: 920 articles)\n",
            "Fetching page 95... (Collected: 930 articles)\n",
            "Fetching page 96... (Collected: 940 articles)\n",
            "Fetching page 97... (Collected: 950 articles)\n",
            "Fetching page 98... (Collected: 960 articles)\n",
            "Fetching page 99... (Collected: 970 articles)\n",
            "Fetching page 100... (Collected: 980 articles)\n",
            "Enter the title (e.g., 'Gautam Adani') to search for in the news: reliance\n",
            "                                 id  \\\n",
            "0  99d33bf42c7f4903aad5b3ab5a41abca   \n",
            "1  1e1ecca5ecc1459d8d644328d21fa8b7   \n",
            "2  e8cafcdc6f72403da0d92140c8276a0e   \n",
            "3  aed24667f924410f8993fb595c8aca39   \n",
            "4  08879c4f1da34a26a385022a8a920c89   \n",
            "\n",
            "                                               title  \\\n",
            "0  Buy Reliance, SBI shares; they're expected to ...   \n",
            "1  Giving JioHotstar domain to Reliance for free:...   \n",
            "2  SEBI sends Reliance Big Entertainment ₹26-cr n...   \n",
            "3  Reliance asks govt to review reach of Starlink...   \n",
            "4  Canara Bank classifies Reliance Communications...   \n",
            "\n",
            "                                            imageUrl                     url  \\\n",
            "0  https://nis-gs.pix.in/inshorts/images/v1/varia...  https://shrts.in/vWHgc   \n",
            "1  https://nis-gs.pix.in/inshorts/images/v1/varia...  https://shrts.in/7Qmne   \n",
            "2  https://nis-gs.pix.in/inshorts/images/v1/varia...  https://shrts.in/1uysq   \n",
            "3  https://nis-gs.pix.in/inshorts/images/v1/varia...  https://shrts.in/zryGi   \n",
            "4  https://nis-gs.pix.in/inshorts/images/v1/varia...  https://shrts.in/eV7Vz   \n",
            "\n",
            "                                             content             author  \\\n",
            "0  Market veteran Nooresh Merani advised investor...  Debaroti Adhikary   \n",
            "1  Dubai-based siblings Jainam and Jivika, who pu...     Pragya Swastik   \n",
            "2  SEBI has asked Reliance Big Entertainment to p...      Mansi Agarwal   \n",
            "3  Mukesh Ambani's Reliance has asked the telecom...      Mansi Agarwal   \n",
            "4  Anil Ambani's Reliance Communications' loan ac...      Mansi Agarwal   \n",
            "\n",
            "                        date      time  \\\n",
            "0  Monday, 18 November, 2024  11:33 am   \n",
            "1  Monday, 18 November, 2024  09:45 am   \n",
            "2  Friday, 15 November, 2024  10:33 pm   \n",
            "3  Friday, 15 November, 2024  09:24 pm   \n",
            "4  Friday, 15 November, 2024  08:41 pm   \n",
            "\n",
            "                                         readMoreUrl  \n",
            "0  https://www.youtube.com/watch?utm_campaign=ful...  \n",
            "1  https://www.hindustantimes.com/business/jiohot...  \n",
            "2  https://www.ptinews.com/story/business/Sebi-sl...  \n",
            "3  https://www.reuters.com/business/media-telecom...  \n",
            "4  https://www.ndtvprofit.com/amp/business/anil-a...  \n",
            "Filtered news data saved to CSV successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iO7YlOgue25i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}